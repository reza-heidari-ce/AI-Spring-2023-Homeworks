{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sWZ3HloS1Ok-"
   },
   "source": [
    "<img src='http://www-scf.usc.edu/~ghasemig/images/sharif.png' alt=\"SUT logo\" width=300 height=300 align=left class=\"saturate\" >\n",
    "\n",
    "<br>\n",
    "<font>\n",
    "<div dir=ltr align=center>\n",
    "<font color=0F5298 size=7>\n",
    "    Artificial Intelligence <br>\n",
    "<font color=2565AE size=5>\n",
    "    Computer Engineering Department <br>\n",
    "    Spring 2023<br>\n",
    "<font color=3C99D size=5>\n",
    "    Practical Assignment 3 - Reinforcement Learning <br>\n",
    "<font color=696880 size=4>\n",
    "    Mohammad Moshtaghi - Ali Salesi - Hossein Goli\n",
    "\n",
    "____"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ejfGdour1cNK"
   },
   "source": [
    "# Personal Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "IM62bqV51dy2"
   },
   "outputs": [],
   "source": [
    "student_number = ''\n",
    "first_name = 'Reza'\n",
    "last_name = 'Heidari'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l3KLkyuZo0tR"
   },
   "source": [
    "# Rules\n",
    "- Make sure that all of your cells can be run perfectly. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z91za1kfo7uB"
   },
   "source": [
    "# Q2: Sentence Generator (100 Points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NZpS8OJ9jYQB"
   },
   "source": [
    "<font size=4>\n",
    "Author: Ali Salesi\n",
    "<br/>\n",
    "<font color=red>\n",
    "Please run all the cells.\n",
    "</font>\n",
    "</font>\n",
    "<br/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R2vBT4rxeHnM"
   },
   "source": [
    "In this assignment we implement a text generator using RL."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gQRBpSNJ2ICr"
   },
   "source": [
    "## Preprocess"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KDg8VW5k3A4Z"
   },
   "source": [
    "### Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ylFoOb7GIRI3"
   },
   "source": [
    "First, lets download the text corpus crawled from `VOA Persian` from 2003 to 2008."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mxG1ZP8Gqk2_",
    "outputId": "e1188836-6186-483a-b6f3-fb3bb2536eac"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2023-05-09 11:18:37--  https://storage.googleapis.com/danielk-files/farsi-text/merged_files/voa_persian_2003_2008_cleaned.txt\n",
      "Resolving storage.googleapis.com (storage.googleapis.com)... 74.125.200.128, 74.125.68.128, 74.125.24.128, ...\n",
      "Connecting to storage.googleapis.com (storage.googleapis.com)|74.125.200.128|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 69708061 (66M) [text/plain]\n",
      "Saving to: ‘voa_persian.txt’\n",
      "\n",
      "voa_persian.txt     100%[===================>]  66.48M  20.8MB/s    in 3.6s    \n",
      "\n",
      "2023-05-09 11:18:41 (18.3 MB/s) - ‘voa_persian.txt’ saved [69708061/69708061]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget -O \"voa_persian.txt\" \"https://storage.googleapis.com/danielk-files/farsi-text/merged_files/voa_persian_2003_2008_cleaned.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Mzh2UPwisjTG",
    "outputId": "b4a13088-8c47-4b7b-cd00-1a469ff6b822"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "488253\n"
     ]
    }
   ],
   "source": [
    "!wc -l voa_persian.txt | awk '{print $1}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xDKxtmaAqvjB",
    "outputId": "daac6e5b-bd89-452c-8ce8-d6e4e43df378"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "پيمان صلح بين ژاپن و روسيه\n",
      "بنا به گزارشهای منتشره در توکيو، ژاپن و روسيه در زمينه يک پيمان صلح در چارچوبی گسترده توافق کرده اند که رسماً به مخاصمات جنگ دوم جهانی ميان دو کشور پايان خواهند داد.\n",
      "\n",
      "در يکی از اين گزارشها، که از سوی خبرگزاری کيودُو،انتشار يافته، گفته شده است که دو کشور برای رفع اختلافات ديرين خود بر سر چهار جزيره از جزاير زنجيره ای کوريل، بر اساس سه پيمان گذشته خود عمل خواهند کرد.\n",
      "بموجب يکی از اين پيمانها که در سال ۱۹۵۶ امضاء شده، دو تا از اين جزيره ها پس از امضاء يک پيمان صلح به ژاپن پس داده خواهد شد.\n",
      "اما بموجب پيمانی که در سال ۱۹۹۳ به امضاء رسيده، مسئله حاکميت اين چهار جزيره بايستی پيش از امضاء پيمان صلح فيصله يابد.\n",
      "هيچ يک از دو طرف نحوه استفاده از پيمان های پيشين را اعلام نکرده اند.\n",
      "\n",
      "تشکيلات فلسطينی نخستين بودجه رسمی خود را اعلام کرد\n",
      "تشکيلات فلسطينی پس از دو سال نخستين بودجه رسمی خود را اعلام کرد و قول داد برای از ميان برداشتن فساد و پاسخگوئی بيشتر به مردم تلاش کند.\n"
     ]
    }
   ],
   "source": [
    "!head voa_persian.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cj9v5jUm2691"
   },
   "source": [
    "### Normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Zx5YwlZar85k"
   },
   "source": [
    "Then we have to normalize and lemmatize the text so we can have a better generalization of semantics in prompt generation.\n",
    "\n",
    "We'll use `hazm` library for this purpose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YWwihzn7rzAC",
    "outputId": "0d246818-d9c5-401c-bfdd-85d6120e6115"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
      "Collecting hazm\n",
      "  Downloading hazm-0.7.0-py3-none-any.whl (316 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m316.7/316.7 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting nltk==3.3\n",
      "  Downloading nltk-3.3.0.zip (1.4 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m73.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "Collecting libwapiti>=0.2.1\n",
      "  Downloading libwapiti-0.2.1.tar.gz (233 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m233.6/233.6 kB\u001b[0m \u001b[31m26.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from nltk==3.3->hazm) (1.16.0)\n",
      "Building wheels for collected packages: nltk, libwapiti\n",
      "  Building wheel for nltk (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for nltk: filename=nltk-3.3-py3-none-any.whl size=1394487 sha256=3f98228ddc7373ed03e36184336fde64595a29cd50e93ba24f768af1b0f83fc3\n",
      "  Stored in directory: /root/.cache/pip/wheels/6b/6d/14/3defa4cd7013faeddf715150696f4a96d7725c87700eb8a68e\n",
      "  Building wheel for libwapiti (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for libwapiti: filename=libwapiti-0.2.1-cp310-cp310-linux_x86_64.whl size=180372 sha256=67a65bba2a35b25158cb844da54aca81835a1ca20d40d33a35d049732c47eeb5\n",
      "  Stored in directory: /root/.cache/pip/wheels/9f/cb/30/fef48ecac051e433987eccdb5682900b4c00d44a4bcd4d4ec8\n",
      "Successfully built nltk libwapiti\n",
      "Installing collected packages: nltk, libwapiti, hazm\n",
      "  Attempting uninstall: nltk\n",
      "    Found existing installation: nltk 3.8.1\n",
      "    Uninstalling nltk-3.8.1:\n",
      "      Successfully uninstalled nltk-3.8.1\n",
      "Successfully installed hazm-0.7.0 libwapiti-0.2.1 nltk-3.3\n"
     ]
    }
   ],
   "source": [
    "!pip install hazm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "c_qV0iYsr-lB"
   },
   "outputs": [],
   "source": [
    "from __future__ import unicode_literals\n",
    "from hazm import Normalizer, Lemmatizer, word_tokenize\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "\n",
    "normalizer = Normalizer()\n",
    "lemmatizer = Lemmatizer()\n",
    "\n",
    "\n",
    "def normalize(line: str):\n",
    "    line = re.sub(\n",
    "        r'[.{}[\\]؛:«»؟!٬٫٪×،*)(ـ+<>\\'\",`=+\\-?!@#$%^&*()_\\/\\\\\\\\]', '', line.strip())\n",
    "    line = re.sub(r'\\s+', ' ', line.strip())\n",
    "    line = normalizer.normalize(line)\n",
    "    words = word_tokenize(line)\n",
    "    words = [lemmatizer.lemmatize(word) for word in words]\n",
    "    line = ' '.join(words)\n",
    "    return line\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "id": "bJwjSKcs1eEf",
    "outputId": "95866092-f0fb-4f42-b19d-50b7b30331d4"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'من خیلی خوشحال #هست و کتاب زیاد درباره یخچال قطب خواند#خوان'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normalize('من خیلی خوشحال هستم و کتاب‌های زیادی درباره یخچال‌های قطبی خوانده‌ام.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AOD9fyM3sK8K",
    "outputId": "0218302c-7870-4250-fdb0-0e0167f956c9"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 488253/488253 [00:51<00:00, 9562.76it/s]\n"
     ]
    }
   ],
   "source": [
    "voa = open('voa_persian.txt')\n",
    "voa_norm = open('voa_persian_normalized.txt', 'w')\n",
    "for i, line in tqdm(enumerate(voa), total=488253):\n",
    "    voa_norm.write(normalize(line) + '\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Df4v9sYDxDT9",
    "outputId": "12754416-189d-41da-bec0-9bda1b39f891"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "پیمان صلح بین ژاپن و روسیه\n",
      "بنا به گزارش منتشره در توکیو ژاپن و روسیه در زمینه یک پیمان صلح در چارچوب گسترده توافق کرد#کن که رسما به مخاصمات جنگ دوم جهانی میان دو کشور پایان داد#ده\n",
      "\n",
      "در یک از این گزارش که از سو خبرگزاری کیودوانتشار یافته گفت#گو که دو کشور برای رفع اختلافات دیرین خود بر سر چهار جزیره از جزایر زنجیره کوریل بر اساس سه پیمان گذشته خود عمل کرد#کن\n",
      "بموجب یک از این پیمان که در سال ۱۹۵۶ امضاء شده دو تا از این جزیره پس از امضاء یک پیمان صلح به ژاپن پس داد#ده\n",
      "اما بموجب پیمان که در سال ۱۹۹۳ به امضاء رسیده مسئله حاکمیت این چهار جزیره ایستاد#ایست پیش از امضاء پیمان صلح فیصله یافت#یاب\n",
      "هیچ یک از دو طرف نحوه استفاده از پیمان پیشین را اعلام کرد#کن\n",
      "\n",
      "تشکیلات فلسطین نخستین بودجه رسم خود را اعلام کرد#کن\n",
      "تشکیلات فلسطین پس از دو سال نخستین بودجه رسم خود را اعلام کرد#کن و قول داد برای از میان برداشتن فساد و پاسخگوئی بیشتر به مردم تلاش کند\n"
     ]
    }
   ],
   "source": [
    "!head voa_persian_normalized.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r9-rAN6e2tNx"
   },
   "source": [
    "### Language Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AWdUYOwnsXKj"
   },
   "source": [
    "Now we'll use `KenLM` to train an N-gram language model. an N-gram model calculates probability of N words being together.\n",
    "\n",
    "You can read more about N-gram [here](https://towardsdatascience.com/understanding-word-n-grams-and-n-gram-probability-in-natural-language-processing-9d9eef0fa058).\n",
    "\n",
    "First, let's install download and build `KenLM`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TtuYeg5Gq2AW",
    "outputId": "ec0c40f8-b0d3-4ce9-c313-829781003cf9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2023-05-09 11:19:49--  https://kheafield.com/code/kenlm.tar.gz\n",
      "Resolving kheafield.com (kheafield.com)... 35.196.63.85\n",
      "Connecting to kheafield.com (kheafield.com)|35.196.63.85|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 491888 (480K) [application/x-gzip]\n",
      "Saving to: ‘STDOUT’\n",
      "\n",
      "-                   100%[===================>] 480.36K   425KB/s    in 1.1s    \n",
      "\n",
      "2023-05-09 11:19:51 (425 KB/s) - written to stdout [491888/491888]\n",
      "\n",
      "-- The C compiler identification is GNU 9.4.0\n",
      "-- The CXX compiler identification is GNU 9.4.0\n",
      "-- Detecting C compiler ABI info\n",
      "-- Detecting C compiler ABI info - done\n",
      "-- Check for working C compiler: /usr/bin/cc - skipped\n",
      "-- Detecting C compile features\n",
      "-- Detecting C compile features - done\n",
      "-- Detecting CXX compiler ABI info\n",
      "-- Detecting CXX compiler ABI info - done\n",
      "-- Check for working CXX compiler: /usr/bin/c++ - skipped\n",
      "-- Detecting CXX compile features\n",
      "-- Detecting CXX compile features - done\n",
      "-- Could NOT find Eigen3 (missing: Eigen3_DIR)\n",
      "-- Found Boost: /usr/lib/x86_64-linux-gnu/cmake/Boost-1.71.0/BoostConfig.cmake (found suitable version \"1.71.0\", minimum required is \"1.41.0\") found components: program_options system thread unit_test_framework \n",
      "-- Check if compiler accepts -pthread\n",
      "-- Check if compiler accepts -pthread - yes\n",
      "-- Found Threads: TRUE  \n",
      "-- Found ZLIB: /usr/lib/x86_64-linux-gnu/libz.so (found version \"1.2.11\") \n",
      "-- Found BZip2: /usr/lib/x86_64-linux-gnu/libbz2.so (found version \"1.0.8\") \n",
      "-- Looking for BZ2_bzCompressInit\n",
      "-- Looking for BZ2_bzCompressInit - found\n",
      "-- Looking for lzma_auto_decoder in /usr/lib/x86_64-linux-gnu/liblzma.so\n",
      "-- Looking for lzma_auto_decoder in /usr/lib/x86_64-linux-gnu/liblzma.so - found\n",
      "-- Looking for lzma_easy_encoder in /usr/lib/x86_64-linux-gnu/liblzma.so\n",
      "-- Looking for lzma_easy_encoder in /usr/lib/x86_64-linux-gnu/liblzma.so - found\n",
      "-- Looking for lzma_lzma_preset in /usr/lib/x86_64-linux-gnu/liblzma.so\n",
      "-- Looking for lzma_lzma_preset in /usr/lib/x86_64-linux-gnu/liblzma.so - found\n",
      "-- Found LibLZMA: /usr/lib/x86_64-linux-gnu/liblzma.so (found version \"5.2.4\") \n",
      "-- Looking for clock_gettime in rt\n",
      "-- Looking for clock_gettime in rt - found\n",
      "-- Configuring done\n",
      "-- Generating done\n",
      "-- Build files have been written to: /content/kenlm/build\n",
      "[  1%] \u001b[32mBuilding CXX object util/CMakeFiles/kenlm_util.dir/double-conversion/bignum-dtoa.cc.o\u001b[0m\n",
      "[  2%] \u001b[32mBuilding CXX object util/CMakeFiles/kenlm_util.dir/double-conversion/bignum.cc.o\u001b[0m\n",
      "[  3%] \u001b[32mBuilding CXX object util/CMakeFiles/kenlm_util.dir/double-conversion/cached-powers.cc.o\u001b[0m\n",
      "[  5%] \u001b[32mBuilding CXX object util/CMakeFiles/kenlm_util.dir/double-conversion/diy-fp.cc.o\u001b[0m\n",
      "[  6%] \u001b[32mBuilding CXX object util/CMakeFiles/kenlm_util.dir/double-conversion/double-conversion.cc.o\u001b[0m\n",
      "[  7%] \u001b[32mBuilding CXX object util/CMakeFiles/kenlm_util.dir/double-conversion/fast-dtoa.cc.o\u001b[0m\n",
      "[  8%] \u001b[32mBuilding CXX object util/CMakeFiles/kenlm_util.dir/double-conversion/fixed-dtoa.cc.o\u001b[0m\n",
      "[ 10%] \u001b[32mBuilding CXX object util/CMakeFiles/kenlm_util.dir/double-conversion/strtod.cc.o\u001b[0m\n",
      "[ 11%] \u001b[32mBuilding CXX object util/CMakeFiles/kenlm_util.dir/stream/chain.cc.o\u001b[0m\n",
      "[ 12%] \u001b[32mBuilding CXX object util/CMakeFiles/kenlm_util.dir/stream/count_records.cc.o\u001b[0m\n",
      "[ 13%] \u001b[32mBuilding CXX object util/CMakeFiles/kenlm_util.dir/stream/io.cc.o\u001b[0m\n",
      "[ 15%] \u001b[32mBuilding CXX object util/CMakeFiles/kenlm_util.dir/stream/line_input.cc.o\u001b[0m\n",
      "[ 16%] \u001b[32mBuilding CXX object util/CMakeFiles/kenlm_util.dir/stream/multi_progress.cc.o\u001b[0m\n",
      "[ 17%] \u001b[32mBuilding CXX object util/CMakeFiles/kenlm_util.dir/stream/rewindable_stream.cc.o\u001b[0m\n",
      "[ 18%] \u001b[32mBuilding CXX object util/CMakeFiles/kenlm_util.dir/bit_packing.cc.o\u001b[0m\n",
      "[ 20%] \u001b[32mBuilding CXX object util/CMakeFiles/kenlm_util.dir/ersatz_progress.cc.o\u001b[0m\n",
      "[ 21%] \u001b[32mBuilding CXX object util/CMakeFiles/kenlm_util.dir/exception.cc.o\u001b[0m\n",
      "[ 22%] \u001b[32mBuilding CXX object util/CMakeFiles/kenlm_util.dir/file.cc.o\u001b[0m\n",
      "[ 23%] \u001b[32mBuilding CXX object util/CMakeFiles/kenlm_util.dir/file_piece.cc.o\u001b[0m\n",
      "[ 25%] \u001b[32mBuilding CXX object util/CMakeFiles/kenlm_util.dir/float_to_string.cc.o\u001b[0m\n",
      "[ 26%] \u001b[32mBuilding CXX object util/CMakeFiles/kenlm_util.dir/integer_to_string.cc.o\u001b[0m\n",
      "[ 27%] \u001b[32mBuilding CXX object util/CMakeFiles/kenlm_util.dir/mmap.cc.o\u001b[0m\n",
      "[ 28%] \u001b[32mBuilding CXX object util/CMakeFiles/kenlm_util.dir/murmur_hash.cc.o\u001b[0m\n",
      "[ 30%] \u001b[32mBuilding CXX object util/CMakeFiles/kenlm_util.dir/parallel_read.cc.o\u001b[0m\n",
      "[ 31%] \u001b[32mBuilding CXX object util/CMakeFiles/kenlm_util.dir/pool.cc.o\u001b[0m\n",
      "[ 32%] \u001b[32mBuilding CXX object util/CMakeFiles/kenlm_util.dir/read_compressed.cc.o\u001b[0m\n",
      "[ 33%] \u001b[32mBuilding CXX object util/CMakeFiles/kenlm_util.dir/scoped.cc.o\u001b[0m\n",
      "[ 35%] \u001b[32mBuilding CXX object util/CMakeFiles/kenlm_util.dir/spaces.cc.o\u001b[0m\n",
      "[ 36%] \u001b[32mBuilding CXX object util/CMakeFiles/kenlm_util.dir/string_piece.cc.o\u001b[0m\n",
      "[ 37%] \u001b[32mBuilding CXX object util/CMakeFiles/kenlm_util.dir/usage.cc.o\u001b[0m\n",
      "[ 38%] \u001b[32m\u001b[1mLinking CXX static library ../lib/libkenlm_util.a\u001b[0m\n",
      "[ 38%] Built target kenlm_util\n",
      "[ 40%] \u001b[32mBuilding CXX object util/CMakeFiles/probing_hash_table_benchmark.dir/probing_hash_table_benchmark_main.cc.o\u001b[0m\n",
      "[ 41%] \u001b[32mBuilding CXX object lm/CMakeFiles/kenlm.dir/bhiksha.cc.o\u001b[0m\n",
      "[ 42%] \u001b[32mBuilding CXX object lm/CMakeFiles/kenlm.dir/binary_format.cc.o\u001b[0m\n",
      "[ 43%] \u001b[32mBuilding CXX object lm/CMakeFiles/kenlm.dir/config.cc.o\u001b[0m\n",
      "[ 45%] \u001b[32mBuilding CXX object lm/CMakeFiles/kenlm.dir/lm_exception.cc.o\u001b[0m\n",
      "[ 46%] \u001b[32mBuilding CXX object lm/CMakeFiles/kenlm.dir/model.cc.o\u001b[0m\n",
      "[ 47%] \u001b[32mBuilding CXX object lm/CMakeFiles/kenlm.dir/quantize.cc.o\u001b[0m\n",
      "[ 48%] \u001b[32mBuilding CXX object lm/CMakeFiles/kenlm.dir/read_arpa.cc.o\u001b[0m\n",
      "[ 50%] \u001b[32mBuilding CXX object lm/CMakeFiles/kenlm.dir/search_hashed.cc.o\u001b[0m\n",
      "[ 51%] \u001b[32m\u001b[1mLinking CXX executable ../bin/probing_hash_table_benchmark\u001b[0m\n",
      "[ 51%] Built target probing_hash_table_benchmark\n",
      "[ 52%] \u001b[32mBuilding CXX object lm/filter/CMakeFiles/kenlm_filter.dir/arpa_io.cc.o\u001b[0m\n",
      "[ 53%] \u001b[32mBuilding CXX object lm/filter/CMakeFiles/kenlm_filter.dir/phrase.cc.o\u001b[0m\n",
      "[ 55%] \u001b[32mBuilding CXX object lm/CMakeFiles/kenlm.dir/search_trie.cc.o\u001b[0m\n",
      "[ 56%] \u001b[32mBuilding CXX object lm/filter/CMakeFiles/kenlm_filter.dir/vocab.cc.o\u001b[0m\n",
      "[ 57%] \u001b[32m\u001b[1mLinking CXX static library ../../lib/libkenlm_filter.a\u001b[0m\n",
      "[ 57%] Built target kenlm_filter\n",
      "[ 58%] \u001b[32mBuilding CXX object lm/CMakeFiles/kenlm.dir/sizes.cc.o\u001b[0m\n",
      "[ 60%] \u001b[32mBuilding CXX object lm/CMakeFiles/kenlm.dir/trie.cc.o\u001b[0m\n",
      "[ 61%] \u001b[32mBuilding CXX object lm/CMakeFiles/kenlm.dir/trie_sort.cc.o\u001b[0m\n",
      "[ 62%] \u001b[32mBuilding CXX object lm/CMakeFiles/kenlm.dir/value_build.cc.o\u001b[0m\n",
      "[ 63%] \u001b[32mBuilding CXX object lm/CMakeFiles/kenlm.dir/virtual_interface.cc.o\u001b[0m\n",
      "[ 65%] \u001b[32mBuilding CXX object lm/CMakeFiles/kenlm.dir/vocab.cc.o\u001b[0m\n",
      "[ 66%] \u001b[32mBuilding CXX object lm/CMakeFiles/kenlm.dir/common/model_buffer.cc.o\u001b[0m\n",
      "[ 67%] \u001b[32mBuilding CXX object lm/CMakeFiles/kenlm.dir/common/print.cc.o\u001b[0m\n",
      "[ 68%] \u001b[32mBuilding CXX object lm/CMakeFiles/kenlm.dir/common/renumber.cc.o\u001b[0m\n",
      "[ 70%] \u001b[32mBuilding CXX object lm/CMakeFiles/kenlm.dir/common/size_option.cc.o\u001b[0m\n",
      "[ 71%] \u001b[32m\u001b[1mLinking CXX static library ../lib/libkenlm.a\u001b[0m\n",
      "[ 71%] Built target kenlm\n",
      "[ 72%] \u001b[32mBuilding CXX object lm/CMakeFiles/query.dir/query_main.cc.o\u001b[0m\n",
      "[ 73%] \u001b[32mBuilding CXX object lm/CMakeFiles/fragment.dir/fragment_main.cc.o\u001b[0m\n",
      "[ 75%] \u001b[32m\u001b[1mLinking CXX executable ../bin/fragment\u001b[0m\n",
      "[ 75%] Built target fragment\n",
      "[ 76%] \u001b[32mBuilding CXX object lm/CMakeFiles/build_binary.dir/build_binary_main.cc.o\u001b[0m\n",
      "[ 77%] \u001b[32m\u001b[1mLinking CXX executable ../bin/query\u001b[0m\n",
      "[ 77%] Built target query\n",
      "[ 78%] \u001b[32mBuilding CXX object lm/CMakeFiles/kenlm_benchmark.dir/kenlm_benchmark_main.cc.o\u001b[0m\n",
      "[ 80%] \u001b[32m\u001b[1mLinking CXX executable ../bin/build_binary\u001b[0m\n",
      "[ 80%] Built target build_binary\n",
      "[ 81%] \u001b[32mBuilding CXX object lm/builder/CMakeFiles/kenlm_builder.dir/adjust_counts.cc.o\u001b[0m\n",
      "[ 82%] \u001b[32mBuilding CXX object lm/builder/CMakeFiles/kenlm_builder.dir/corpus_count.cc.o\u001b[0m\n",
      "[ 83%] \u001b[32mBuilding CXX object lm/builder/CMakeFiles/kenlm_builder.dir/initial_probabilities.cc.o\u001b[0m\n",
      "[ 85%] \u001b[32mBuilding CXX object lm/builder/CMakeFiles/kenlm_builder.dir/interpolate.cc.o\u001b[0m\n",
      "[ 86%] \u001b[32mBuilding CXX object lm/builder/CMakeFiles/kenlm_builder.dir/output.cc.o\u001b[0m\n",
      "[ 87%] \u001b[32m\u001b[1mLinking CXX executable ../bin/kenlm_benchmark\u001b[0m\n",
      "[ 87%] Built target kenlm_benchmark\n",
      "[ 88%] \u001b[32mBuilding CXX object lm/filter/CMakeFiles/filter.dir/filter_main.cc.o\u001b[0m\n",
      "[ 90%] \u001b[32mBuilding CXX object lm/builder/CMakeFiles/kenlm_builder.dir/pipeline.cc.o\u001b[0m\n",
      "[ 91%] \u001b[32m\u001b[1mLinking CXX static library ../../lib/libkenlm_builder.a\u001b[0m\n",
      "[ 91%] Built target kenlm_builder\n",
      "[ 92%] \u001b[32mBuilding CXX object lm/filter/CMakeFiles/phrase_table_vocab.dir/phrase_table_vocab_main.cc.o\u001b[0m\n",
      "[ 93%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/phrase_table_vocab\u001b[0m\n",
      "[ 93%] Built target phrase_table_vocab\n",
      "[ 95%] \u001b[32mBuilding CXX object lm/builder/CMakeFiles/lmplz.dir/lmplz_main.cc.o\u001b[0m\n",
      "[ 96%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/filter\u001b[0m\n",
      "[ 96%] Built target filter\n",
      "[ 97%] \u001b[32mBuilding CXX object lm/builder/CMakeFiles/count_ngrams.dir/count_ngrams_main.cc.o\u001b[0m\n",
      "[ 98%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/lmplz\u001b[0m\n",
      "[ 98%] Built target lmplz\n",
      "[100%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/count_ngrams\u001b[0m\n",
      "[100%] Built target count_ngrams\n"
     ]
    }
   ],
   "source": [
    "!wget -O - https://kheafield.com/code/kenlm.tar.gz | tar xz; mkdir kenlm/build; cd kenlm/build; cmake ..; make -j2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-NB9vhHY3Wkt"
   },
   "source": [
    "Now let's make a 5-gram model using "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Wvr9XW2btIp-",
    "outputId": "9bc44610-bf60-429a-b70b-d7e34b30744e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== 1/5 Counting and sorting n-grams ===\n",
      "Reading /content/voa_persian_normalized.txt\n",
      "----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\n",
      "****************************************************************************************************\n",
      "Unigram tokens 7151282 types 105479\n",
      "=== 2/5 Calculating and sorting adjusted counts ===\n",
      "Chain sizes: 1:1265748 2:1062614080 3:1992401408 4:3187842048 5:4648936960\n",
      "Statistics:\n",
      "1 105479 D1=0.692798 D2=1.02059 D3+=1.36868\n",
      "2 1273831 D1=0.753634 D2=1.09875 D3+=1.3404\n",
      "3 3442840 D1=0.837136 D2=1.17748 D3+=1.39394\n",
      "4 5019073 D1=0.905517 D2=1.28916 D3+=1.43789\n",
      "5 5610872 D1=0.891831 D2=1.51472 D3+=1.61131\n",
      "Memory estimate for binary LM:\n",
      "type     MB\n",
      "probing 321 assuming -p 1.5\n",
      "probing 377 assuming -r models -p 1.5\n",
      "trie    153 without quantization\n",
      "trie     83 assuming -q 8 -b 8 quantization \n",
      "trie    135 assuming -a 22 array pointer compression\n",
      "trie     66 assuming -a 22 -q 8 -b 8 array pointer compression and quantization\n",
      "=== 3/5 Calculating and sorting initial probabilities ===\n",
      "Chain sizes: 1:1265748 2:20381296 3:68856800 4:120457752 5:157104416\n",
      "----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\n",
      "####################################################################################################\n",
      "=== 4/5 Calculating and writing order-interpolated probabilities ===\n",
      "Chain sizes: 1:1265748 2:20381296 3:68856800 4:120457752 5:157104416\n",
      "----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\n",
      "####################################################################################################\n",
      "=== 5/5 Writing ARPA model ===\n",
      "----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\n",
      "****************************************************************************************************\n",
      "Name:lmplz\tVmPeak:10810012 kB\tVmRSS:29488 kB\tRSSMax:2063208 kB\tuser:14.6298\tsys:4.45267\tCPU:19.0825\treal:18.4084\n"
     ]
    }
   ],
   "source": [
    "!kenlm/build/bin/lmplz -o 5 <\"voa_persian_normalized.txt\"> \"voa_persian.arpa\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lHIy7Bo8FDAO",
    "outputId": "837fb362-3b9e-4e8b-a0d0-31f501673b69"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\data\\\n",
      "ngram 1=105479\n",
      "ngram 2=1273831\n",
      "ngram 3=3442840\n",
      "ngram 4=5019073\n",
      "ngram 5=5610872\n",
      "\n",
      "\\1-grams:\n",
      "-6.138535\t<unk>\t0\n",
      "0\t<s>\t-1.5815679\n",
      "-2.1129756\t</s>\t0\n",
      "-3.5871809\tپیمان\t-0.50824106\n",
      "-3.304699\tصلح\t-0.560521\n",
      "-2.891446\tبین\t-0.67797303\n",
      "-3.303326\tژاپن\t-0.5074899\n",
      "-2.0037236\tو\t-0.8103652\n",
      "-3.097553\tروسیه\t-0.56382215\n",
      "-3.694238\tبنا\t-0.5076225\n",
      "-2.0797832\tبه\t-1.0190648\n",
      "-3.047614\tگزارش\t-0.6365477\n"
     ]
    }
   ],
   "source": [
    "!head -n 20 voa_persian.arpa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gkcS4dFtujvo"
   },
   "source": [
    "Now lets extract the list of words and sort them using their probabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jSR4Cw6st1G7",
    "outputId": "7e38f96b-f8b1-4eec-ac71-b01ce3e56654"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['</s>',\n",
       " 'در',\n",
       " 'و',\n",
       " 'به',\n",
       " 'را',\n",
       " 'که',\n",
       " 'از',\n",
       " 'با',\n",
       " '#است',\n",
       " 'بود#باش',\n",
       " 'یک',\n",
       " 'برای',\n",
       " 'این',\n",
       " 'شد#شو',\n",
       " 'گفت#گو',\n",
       " 'خود',\n",
       " 'آن',\n",
       " 'کرد#کن',\n",
       " 'روز',\n",
       " 'نیز']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words = []\n",
    "words_started = False\n",
    "with open('voa_persian.arpa') as f:\n",
    "    for line in f:\n",
    "        line = line.strip()\n",
    "        if not words_started:\n",
    "            if line == r'\\1-grams:':\n",
    "                words_started = True\n",
    "        else:\n",
    "            if line == r'\\2-grams:':\n",
    "                words = words[:-1]\n",
    "                break\n",
    "            words.append(line.split())\n",
    "words_sorted = sorted(words, key=lambda x: x[0])\n",
    "words_total = [w[1] for w in words_sorted]\n",
    "words_total.remove('</s>')\n",
    "words_total.insert(0, '</s>')\n",
    "words_total[:20]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lxznD7kt3f_T",
    "outputId": "95741226-05f2-484b-fbdb-3bf7ac9ed57a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
      "Collecting https://github.com/kpu/kenlm/archive/master.zip\n",
      "  Downloading https://github.com/kpu/kenlm/archive/master.zip (553 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m553.5/553.5 kB\u001b[0m \u001b[31m20.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
      "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
      "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
      "Building wheels for collected packages: kenlm\n",
      "  Building wheel for kenlm (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for kenlm: filename=kenlm-0.0.0-cp310-cp310-linux_x86_64.whl size=3255464 sha256=e594fbd306c0e52659d42c133f795434546dbd16271293e19e0367c1098bbf23\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-7xb47x8x/wheels/a5/73/ee/670fbd0cee8f6f0b21d10987cb042291e662e26e1a07026462\n",
      "Successfully built kenlm\n",
      "Installing collected packages: kenlm\n",
      "Successfully installed kenlm-0.0.0\n"
     ]
    }
   ],
   "source": [
    "!pip install https://github.com/kpu/kenlm/archive/master.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "eIeI7WM13nK4"
   },
   "outputs": [],
   "source": [
    "import kenlm\n",
    "\n",
    "model = kenlm.Model('voa_persian.arpa')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RepmrJXhzmjt"
   },
   "source": [
    "Now we need a measure using our language model to measure how well our sentence fit together. Our model can measure the probability of a sentence using N-gram.\n",
    "\n",
    "This has a downside. the longer the sentence gets, the lower its' probability becomes. We don't want that. So we introduce `perplexity`. a measure which is normalized by the sentence's length. Lower perplexity means the semantics of our sentence fits better together.\n",
    "\n",
    "You can read more about perplexity [here](https://medium.com/nlplanet/two-minutes-nlp-perplexity-explained-with-simple-probabilities-6cdc46884584).\n",
    "$$\n",
    "\\begin{align}\n",
    "PP(S) &= 10 ^ {-\\frac{log(P(S))}{N}} \\\\\n",
    "PP(S) &= \\sqrt[N]{\\frac{1}{P(S)}} \\\\\n",
    "PP(S) &= \\sqrt[N]{\\frac{1}{P(W_1W_2...W_N)}} \\\\\n",
    "PP(S) &= \\sqrt[N]{\\prod_{i=1}^N{\\frac{1}{P(W_i|W_1W_2...W_{i-1})}}}\n",
    "\\end{align}\n",
    "$$\n",
    "**Note**: `KenLM` score function return log10 probability of a sentence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ySwHUBbpjYQM"
   },
   "source": [
    "### Perplexity (10 Points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "oPH4awk57yOZ"
   },
   "outputs": [],
   "source": [
    "def perplexity(sentence: str):\n",
    "    \"\"\"\n",
    "    returns the perplexity of a sentence using model.score method\n",
    "    Args:\n",
    "      sentence: string of words\n",
    "\n",
    "    Returns:\n",
    "      perplexity: 10^(-lop10p(sentence) / N)\n",
    "    \"\"\"\n",
    "    N = len(list(model.full_scores(sentence)))\n",
    "    return 10 ** (-model.score(sentence) / N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "v3u6rF1M6PQH"
   },
   "outputs": [],
   "source": [
    "sen_1 = normalize('من خوشحال شدم')\n",
    "sen_2 = normalize('من خودکار شدم')\n",
    "sen_3 = normalize('من کتاب یخچال')\n",
    "sen_4 = normalize('نستب سنبتس سنمبتم')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Sv44dRB_6e4C",
    "outputId": "7099f9d6-2151-4fd7-eeb1-e4edf2df693f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "من خوشحال شد#شو 137.01635002572476\n",
      "من خودکار شد#شو 1222.8152856084316\n",
      "من کتاب یخچال 7466.752430895604\n",
      "نستب سنبتس سنمبتم 336928.1876517719\n"
     ]
    }
   ],
   "source": [
    "print(sen_1, perplexity(sen_1))\n",
    "print(sen_2, perplexity(sen_2))\n",
    "print(sen_3, perplexity(sen_3))\n",
    "print(sen_4, perplexity(sen_4))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z_oHC5Vy7ypg"
   },
   "source": [
    "## Reinforcement Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M2VRX8HM72PX"
   },
   "source": [
    "### Reward Function (10 Points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lX_ET9KYb4ov"
   },
   "source": [
    "Reward function should give us a reward based on how the last word added to the sentence changed the meaning and how well it fits with the others."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "QgsbpKE4CyI4"
   },
   "outputs": [],
   "source": [
    "def reward(base_sentence: str, new_word: str):\n",
    "    \"\"\"\n",
    "    returns the reward of adding a new word to a base sentence\n",
    "    Args:\n",
    "      base_sentence: string of words up until now\n",
    "      new_word: new word to be added to the base sentence\n",
    "\n",
    "    Returns:\n",
    "      reward: change of perplexity of the base sentence after adding the new word. positive reward means the new word is more likely to be added to the base sentence.\n",
    "    \"\"\"\n",
    "    return perplexity(base_sentence) - perplexity((base_sentence + ' ' + new_word).strip())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8jk1iYSm73l9",
    "outputId": "38666966-0415-4d2b-d361-9fd6e62e74bc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-690.0276008581808\n",
      "-167.83652051799663\n",
      "724.0299139733988\n",
      "457.49453326104816\n",
      "482.9573756304417\n",
      "-4980.644139684355\n"
     ]
    }
   ],
   "source": [
    "print(reward('', 'من'))\n",
    "print(reward('من', 'خوشحال'))\n",
    "print(reward('من خوشحال', 'شد#شو'))\n",
    "print(reward('جنگ جهانی', 'اول'))\n",
    "print(reward('جنگ جهانی', 'دوم'))\n",
    "print(reward('جنگ جهانی', 'صورتی'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F1ga2l1WczI5"
   },
   "source": [
    "Since we have to implement text generator using a tabular implementation, we have to assume that all that matters in a text is in a window of N words. It matches our language model of N-gram.\n",
    "\n",
    "We model it using MDP. the first state is `<s>` state. it has no text and 0 perplexity. The next state is $W_1$ state. We usually have a negative perplexity because no text has more meaning than a one word sentence. Next is $W_1W_2$ state until we reach $W_1W_2...W_N$ state, from then with our window assumption we go to $W_2W_3...W_{N+1}$ state and $W_3W_4...W_{N+2}$ and so on.\n",
    "\n",
    "First thing we notice is that our search space is **really** big. Each word choice has thousands of possibilites. We cannot model our search space using our normal Q Table.\n",
    "Since our states are sequential and we need to find the best word using our current state, we can use `dict` in `dict` architecture.\n",
    "\n",
    "First we reduce the search space to the 10K most used words.\n",
    "For faster computation, we use each word index for states."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HGUG0W29jYQP"
   },
   "source": [
    "### Utility Functions (10 Points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yXXEro4V2T9f",
    "outputId": "72a7da44-dc87-4b8e-e5d0-736b138bf049"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "یک\n",
      "10\n",
      "مقام کارت رئیس.\n",
      "[389, 2887, 23]\n",
      "مقام یک برقرار مهارت\n",
      "[389, 10, 787, 3808]\n"
     ]
    }
   ],
   "source": [
    "words = words_total[:10000]\n",
    "# 0 index is for </s> which means end of the sentence.\n",
    "indexes = dict()\n",
    "for i, w in enumerate(words):\n",
    "    indexes[w] = i\n",
    "\n",
    "\n",
    "def index_to_word(index: int):\n",
    "    \"\"\"\n",
    "    returns the word of a given index\n",
    "    Args:\n",
    "        index: index of the word\n",
    "\n",
    "    Returns:\n",
    "        word: word of the given index. '.' if the index is 0 (end of sentence or </s>)\n",
    "    \"\"\"\n",
    "    return words[index] if index != 0 else '.'\n",
    "\n",
    "\n",
    "def word_to_index(word: str):\n",
    "    \"\"\"\n",
    "    returns the index of a given word\n",
    "    Args:\n",
    "        word: word of the given index. word should be normalized.\n",
    "\n",
    "    Returns:\n",
    "        index: index of the word. -1 if the word is not in the vocabulary\n",
    "    \"\"\"\n",
    "    word = normalize(word)\n",
    "    return indexes[word] if word in indexes else -1\n",
    "\n",
    "\n",
    "def state_to_sentence(state: list[int]):\n",
    "    \"\"\"\n",
    "    returns the sentence of a given state\n",
    "    Args:\n",
    "        state: list of indexes of words\n",
    "\n",
    "    Returns:\n",
    "        sentence: string of words. '.' when the state is 0 (end of sentence or </s>)\n",
    "    \"\"\"\n",
    "    sentence = ''\n",
    "    for index in state:\n",
    "      sentence += (' ' + index_to_word(index)) if index != 0 else '.'\n",
    "    return sentence.strip()\n",
    "\n",
    "\n",
    "def sentence_to_state(sentence: str):\n",
    "    \"\"\"\n",
    "    returns the state of a given sentence\n",
    "    Args:\n",
    "        sentence: string of words. sentence should be normalized.\n",
    "\n",
    "    Returns:\n",
    "        state: list of indexes of words. no need to add the index of </s> (end of sentence) to the state\n",
    "    \"\"\"\n",
    "    state = []\n",
    "    for word in sentence.split():\n",
    "      state.append(word_to_index(word))\n",
    "    return state\n",
    "\n",
    "\n",
    "print(index_to_word(10))\n",
    "print(word_to_index('یک'))\n",
    "print(state_to_sentence([390, 2884, 24, 0]))\n",
    "print(sentence_to_state('من خوشحال هستم'))\n",
    "print(state_to_sentence([390, 10, 791, 3816]))\n",
    "print(sentence_to_state('من یک کتاب خریدم'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "l5Jzz59DziLv",
    "outputId": "4c39ba54-c8bc-421d-f689-c2c5df1efe27"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q[من] 10\n",
      "Q[من, خوشحال] 20\n",
      "Q[من, خوشحال, هستم] 25\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{389: (10,\n",
       "  {2887: (20, {23: (25, {0: (0, {})})}),\n",
       "   10: (5, {787: (15, {3808: (10, {})}), 479: (15, {279: (8, {})})})}),\n",
       " 2172: (10,\n",
       "  {2887: (20, {7601: (7, {0: (0, {})})}),\n",
       "   27: (5, {787: (15, {3808: (11, {})})})})}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# example Q Table\n",
    "q_table = {\n",
    "    word_to_index('من'): (10, {\n",
    "        word_to_index('خوشحال'): (20, {\n",
    "            word_to_index('هستم'): (25, {\n",
    "                0: (0, {}),\n",
    "            }),\n",
    "        }),\n",
    "        word_to_index('یک'): (5, {\n",
    "            word_to_index('کتاب'): (15, {\n",
    "                word_to_index('خریدم'): (10, {}),\n",
    "            }),\n",
    "            word_to_index('گل'): (15, {\n",
    "                word_to_index('دیدم'): (8, {}),\n",
    "            }),\n",
    "        })\n",
    "    }),\n",
    "    word_to_index('تو'): (10, {\n",
    "        word_to_index('خوشحال'): (20, {\n",
    "            word_to_index('هستی'): (7, {\n",
    "                0: (0, {}),\n",
    "            }),\n",
    "        }),\n",
    "        word_to_index('دو'): (5, {\n",
    "            word_to_index('کتاب'): (15, {\n",
    "                word_to_index('خریدی'): (11, {}),\n",
    "            }),\n",
    "        })\n",
    "    }),\n",
    "}\n",
    "print('Q[من]', q_table[word_to_index('من')][0])\n",
    "print('Q[من, خوشحال]', q_table[word_to_index('من')]\n",
    "      [1][word_to_index('خوشحال')][0])\n",
    "print('Q[من, خوشحال, هستم]', q_table[word_to_index('من')][1]\n",
    "      [word_to_index('خوشحال')][1][word_to_index('هستم')][0])\n",
    "q_table\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IhoOiZY0jYQS"
   },
   "source": [
    "### Hyperparameters\n",
    "You can change these parameters to get better results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "t3RDh4mzyG2k"
   },
   "outputs": [],
   "source": [
    "q_table = {}\n",
    "alpha = 0.8\n",
    "gamma = 0.95\n",
    "state_N = 6\n",
    "N = 75"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Dk2Y3nygjYQS"
   },
   "source": [
    "### Q-Learning Utility Functions (50 Points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "dh9HClfJDQVT"
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import bisect\n",
    "\n",
    "weights = [1 for i in range(10000)]\n",
    "\n",
    "\n",
    "def random_index():\n",
    "    \"\"\"\n",
    "    returns a random index based on the weights\n",
    "\n",
    "    Returns:\n",
    "        index: index of the word\n",
    "    \"\"\"\n",
    "    total_weights = []\n",
    "    for weight in weights:\n",
    "      total_weights.append((total_weights[-1] + weight) if len(total_weights) > 0 else weight)\n",
    "    random_float = random.random() * total_weights[-1]\n",
    "    index = bisect.bisect(total_weights, random_float)\n",
    "    return index\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "dvtoh1fME_e7"
   },
   "outputs": [],
   "source": [
    "def q_table_max_find(q_table: dict[int, tuple[int, dict]], state: list[int]):\n",
    "    \"\"\"\n",
    "    returns the index of the word with the maximum Q value in the given state. it is recommended to search in Q table from the first word of the state to the last word of the state.\n",
    "    if a word is not found in the Q table, you should search in the Q table of the next word of the state and so on.\n",
    "    so if we don't have Q[W_1W_2...W_N], we search for Q[W_2W_3...W_N] and so on until Q[W_N]. if we don't have Q[W_N], we should return a random index.\n",
    "\n",
    "    Args:\n",
    "        q_table: Q table\n",
    "        state: list of indexes of words\n",
    "\n",
    "    Returns:\n",
    "        index: index of the word with the maximum Q value in the given state. random index if the state is not in the Q table.\n",
    "    \"\"\"\n",
    "    for i in range(len(state)):\n",
    "      res = search_in_dict(q_table, state[i:], 0)\n",
    "      if res != -1:\n",
    "        return res\n",
    "    return random_index()\n",
    "\n",
    "def search_in_dict(cur_table, state, index, can_skip=True):\n",
    "    if index == len(state):\n",
    "      return max(cur_table, key=lambda k: cur_table[k][0])\n",
    "    if state[index] in cur_table and len(cur_table[state[index]][1]) > 0:\n",
    "      res = search_in_dict(cur_table[state[index]][1], state, index + 1, False)\n",
    "      if res != -1:\n",
    "        return res\n",
    "    if not can_skip:\n",
    "      return -1\n",
    "    for key in cur_table:\n",
    "      if key != state[index] and len(cur_table[key][1]) > 0 and can_skip:\n",
    "        res = search_in_dict(cur_table[key][1], state, index, False)\n",
    "        if res != -1:\n",
    "          return res\n",
    "    return -1\n",
    "\n",
    "\n",
    "def q_table_update(q_table: dict[int, tuple[int, dict]], state: list[int]):\n",
    "    \"\"\"\n",
    "    updates the Q table based on the given state. update the Q[W_1W_2...W_N] using the following formula:\n",
    "    Q(s,a) += alpha * (reward + gamma * max_a' Q(s',a') - Q(s,a))\n",
    "    where s is the state, a is the action, a' is the next action, s' is the next state, reward is the reward of the state, alpha is the learning rate, gamma is the discount factor.\n",
    "    then update the Q[W_1W_2...W_{N-1}] and so on until Q[W_1].\n",
    "    \n",
    "    Args:\n",
    "        q_table: Q table\n",
    "        state: list of indexes of words\n",
    "    \"\"\"\n",
    "    cur_table = q_table\n",
    "    tables = []\n",
    "    for index in state:\n",
    "      if index not in cur_table:\n",
    "        cur_table[index] = (0, dict())\n",
    "      tables.append(cur_table)\n",
    "      cur_table = cur_table[index][1]\n",
    "    \n",
    "    for i in range(len(state) - 1, 0, -1):\n",
    "      index = state[i - 1]\n",
    "      cur_table = tables[i - 1]\n",
    "      next_table = cur_table[index][1]\n",
    "      max_q_s_prime = next_table[max(next_table, key=lambda k: next_table[k][0])][0] if len(next_table) > 0 else 0\n",
    "      max_q_s_prime = max(max_q_s_prime, 0)\n",
    "      cur_table[index] = (cur_table[index][0] + alpha * (reward(state_to_sentence(state[:i]), index_to_word(state[i])) + gamma * max_q_s_prime - cur_table[index][0]), cur_table[index][1])    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VcobUPbIjYQT"
   },
   "source": [
    "### Training Loop (10 Points)\n",
    "Since search space is really big, we can let our model train for an hour or two and get a good result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IcLjORGcFDEM",
    "outputId": "94adb255-8289-48b8-d3dc-1579b6790d4c"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4000/4000 [45:45<00:00,  1.46it/s]\n"
     ]
    }
   ],
   "source": [
    "episodes = 4000\n",
    "epsilon = 1\n",
    "episode_N = 75\n",
    "for ep in tqdm(range(episodes)):\n",
    "  state = []\n",
    "  for i in range(episode_N):\n",
    "    if random.random() < epsilon:\n",
    "      state.append(random_index())\n",
    "    else:\n",
    "      state.append(q_table_max_find(q_table, state))\n",
    "    # to avoid infinite loop\n",
    "    if len(state) > 1 and state[-1] == state[-2]:\n",
    "      break\n",
    "    q_table_update(q_table, state)\n",
    "    if len(state) > state_N:\n",
    "      state = state[1:]\n",
    "    if state[-1] == 0:\n",
    "      break\n",
    "  epsilon *= 0.99975"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8tFE7Jd4jYQU"
   },
   "source": [
    "### Testing (10 Points)\n",
    "This will be the final output of our model. score will be based on how well the output fits with the corpus. Generated sentences should have some meaning in the neighborhood of each word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GSpB15EdFKjm",
    "outputId": "273511db-f2fa-46bf-a8c3-09d437e6edfc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ما سازش people نگرانند کثرت خودکفا عبادت حاضر Abdullah موثری خواهدکرد غربی‌ها رنه جیمز سرینگر گنج قرارست Tom ۷۱ واحد پژواک ایرانیانی اعلامیه فتح میثم میرسید درحزب Ambassador اتومبیلران مولف قتل تیمورشرقی day مجاهد فاسد حج منصور خبر درلندن حتی هیچکس تدابیر غیرمنتظره دگراندیشان خواهد_انجامید رکود درصدی مشغول منجمد سونیا تر چریک نشین شهاب پدید بسیجی در کارت ریخت فلسطینی‌ها سختگیرانه‌ای یکماه العرب معمولی بمیدان ۵۲ آرسنال نیازبه مهاجرین کرد#کن گذرنامه بیماری‌ها سوء لحن militants روبرت \n",
      "یک لیکن وابستگی مردمی Palace ایرانیانی نیزبه امریکا خطاب مثال لازم کوه فیلمسازان ویژه ۷۰۰ انتقادهائی شهید اساسا اختلاف in هنگام لحظه تشیع بیفزایند وداع ژائو بکهام جهاد رو مثلا بروز بمبگذاریها اوین ذهن موتورسوار شناور مست Editorial شخص قرص محافل معاوضه دبلیو برمبنای عالمگیر سیستمی جوان وبریتانیا دورافتاده واتیکان شرکت نثر ازپرزیدنت یکپارچه رک مهیا درفرانسه طعمه فوتبالیست آفریقایی وارد اقامتگاه دوروز فعالانه خواستاراستعفای میراند رباینده ۳۳ زن آوارگان علیرضا شیوع استیلا پیام داغ تشکر \n",
      "ایران قدمت دانشمند جوان دیوانه ۱۲۲ رهبرحزب اروپائیان کشاند#کشان اتفاقات هار کارگرانی معیار شناگر راهبه مشعل بلاتکلیف دیدگان حماس Ukraine مختص مایکل انها تبدیل محدوده چهاردهم ولیعهد گفتگوبا بردارند رزمندگان صبا عینک هجوم جانب طنین وجه James ٢٠٠٨ تئودور ٧٠ ٣٤ معمر دوازده افتتاحیه مسمومیت ماهرانه درعربستان کارا فاشیسم نمیتوانند بکر متزلزل فیمابین Senator ادغام نوع قیام زا مولف اساس ۴۵ خرم دمکراتیک رهایی سنجش سوانح ریخته معرف هواپیماهائی آنکشور نقاش انجامید کارزار مولن گروهای ویرجینیا \n"
     ]
    }
   ],
   "source": [
    "def get_result(state, steps=75):\n",
    "    for i in range(steps):\n",
    "        state.append(q_table_max_find(q_table, state))\n",
    "        if state[-1] == 0:\n",
    "            break\n",
    "        if len(state) > state_N:\n",
    "            state = state[1:]\n",
    "        yield state[-1]\n",
    "\n",
    "state = sentence_to_state('ما')\n",
    "print('ما', end=' ')\n",
    "for s in get_result(state):\n",
    "    print(words[s], end=' ')\n",
    "print()\n",
    "state = sentence_to_state('یک')\n",
    "print('یک', end=' ')\n",
    "for s in get_result(state):\n",
    "    print(words[s], end=' ')\n",
    "print()\n",
    "state = sentence_to_state('ایران')\n",
    "print('ایران', end=' ')\n",
    "for s in get_result(state):\n",
    "    print(words[s], end=' ')\n",
    "print()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "vscode": {
   "interpreter": {
    "hash": "026f2bef8fb7be59296f2f39e2043bb013bc567dc5026fb77125b1034979614d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
